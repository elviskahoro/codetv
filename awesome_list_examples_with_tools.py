#!/usr/bin/env python3
"""
Comprehensive example demonstrating all Awesome List Agent tools working together.

This script shows how to use:
1. Awesome List Parser Tool
2. YouTube Metadata Tool  
3. Web Scraping Tool
4. Content Analysis Tool

Each tool includes Galileo logging for observability.
"""

import asyncio
import os
import sys
from pathlib import Path

# Add the project root to the Python path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from awesome_list_agent.factory import AwesomeListAgentFactory
from awesome_list_agent.utils.logging import setup_logging


async def demonstrate_awesome_list_parser():
    """Demonstrate the Awesome List Parser tool."""
    print("\n" + "="*60)
    print("DEMONSTRATING AWESOME LIST PARSER TOOL")
    print("="*60)
    
    # Create agent with Galileo logging enabled
    agent = await AwesomeListAgentFactory.create_agent(
        verbosity="high",
        enable_galileo=True
    )
    
    # Test URL - a real awesome list
    test_url = "https://github.com/sindresorhus/awesome"
    
    print(f"Processing Awesome List: {test_url}")
    
    try:
        # Use the awesome list parser tool directly
        parser_tool = agent.parser
        result = await parser_tool.execute(test_url)
        
        if isinstance(result, dict) and "error" not in result:
            print("‚úÖ Successfully parsed Awesome List!")
            print(f"üìã Topic: {result.get('topic', 'N/A')}")
            print(f"üìù Description: {result.get('description', 'N/A')[:100]}...")
            print(f"üìä Total Items: {result.get('total_items', 0)}")
            print(f"üè∑Ô∏è  Categories: {len(result.get('categories', []))}")
            print(f"üíª Language: {result.get('language', 'N/A')}")
            print(f"üìÑ Context Summary: {result.get('context_summary', 'N/A')[:200]}...")
        else:
            print(f"‚ùå Error parsing Awesome List: {result.get('error', 'Unknown error')}")
            
    except Exception as e:
        print(f"‚ùå Exception occurred: {str(e)}")


async def demonstrate_youtube_metadata_tool():
    """Demonstrate the YouTube Metadata tool."""
    print("\n" + "="*60)
    print("DEMONSTRATING YOUTUBE METADATA TOOL")
    print("="*60)
    
    # Create agent with Galileo logging enabled
    agent = await AwesomeListAgentFactory.create_agent(
        verbosity="high",
        enable_galileo=True
    )
    
    # Test YouTube URL
    test_url = "https://www.youtube.com/watch?v=dQw4w9WgXcQ"
    
    print(f"Extracting metadata from: {test_url}")
    
    try:
        # Use the YouTube metadata tool directly
        youtube_tool = agent.youtube_tool
        result = await youtube_tool.execute(test_url, extract_comments=True)
        
        if isinstance(result, dict) and "error" not in result:
            print("‚úÖ Successfully extracted YouTube metadata!")
            print(f"üé¨ Video ID: {result.get('video_id', 'N/A')}")
            print(f"üì∫ Title: {result.get('title', 'N/A')}")
            print(f"üìù Description: {result.get('description', 'N/A')[:100]}...")
            print(f"‚è±Ô∏è  Duration: {result.get('duration', 'N/A')} ({result.get('duration_seconds', 0)} seconds)")
            print(f"üëÅÔ∏è  Views: {result.get('view_count', 0):,}")
            print(f"üëç Likes: {result.get('like_count', 0):,}")
            print(f"üí¨ Comments: {result.get('comment_count', 0):,}")
            print(f"üìÖ Upload Date: {result.get('upload_date', 'N/A')}")
            print(f"üì∫ Channel: {result.get('channel_name', 'N/A')}")
            print(f"üè∑Ô∏è  Tags: {', '.join(result.get('tags', [])[:5])}")
            print(f"üìä Summary: {result.get('metadata_summary', 'N/A')}")
        else:
            print(f"‚ùå Error extracting YouTube metadata: {result.get('error', 'Unknown error')}")
            
    except Exception as e:
        print(f"‚ùå Exception occurred: {str(e)}")


async def demonstrate_web_scraping_tool():
    """Demonstrate the Web Scraping tool."""
    print("\n" + "="*60)
    print("DEMONSTRATING WEB SCRAPING TOOL")
    print("="*60)
    
    # Create agent with Galileo logging enabled
    agent = await AwesomeListAgentFactory.create_agent(
        verbosity="high",
        enable_galileo=True
    )
    
    # Test URL - a simple webpage
    test_url = "https://httpbin.org/html"
    
    print(f"Scraping content from: {test_url}")
    
    try:
        # Use the web scraping tool directly
        scraping_tool = agent.web_scraping_tool
        result = await scraping_tool.execute(
            test_url,
            extract_text=True,
            extract_links=True,
            extract_images=True,
            extract_metadata=True,
            max_links=10,
            max_images=5
        )
        
        if isinstance(result, dict) and "error" not in result:
            print("‚úÖ Successfully scraped web content!")
            print(f"üìÑ Title: {result.get('title', 'N/A')}")
            print(f"üìä Content Type: {result.get('content_type', 'N/A')}")
            print(f"üìè Content Length: {result.get('content_length', 0):,} characters")
            print(f"üìù Text Summary: {result.get('text_summary', 'N/A')}")
            print(f"üîó Links Found: {len(result.get('links', []))}")
            print(f"üñºÔ∏è  Images Found: {len(result.get('images', []))}")
            print(f"üìã Metadata Fields: {len(result.get('metadata', {}))}")
            print(f"üìä Scraping Summary: {result.get('scraping_summary', 'N/A')}")
            
            # Show first few links
            links = result.get('links', [])
            if links:
                print("\nüîó Sample Links:")
                for i, link in enumerate(links[:3]):
                    print(f"  {i+1}. {link.get('text', 'No text')} -> {link.get('url', 'No URL')}")
                    
        else:
            print(f"‚ùå Error scraping web content: {result.get('error', 'Unknown error')}")
            
    except Exception as e:
        print(f"‚ùå Exception occurred: {str(e)}")


async def demonstrate_content_analysis_tool():
    """Demonstrate the Content Analysis tool."""
    print("\n" + "="*60)
    print("DEMONSTRATING CONTENT ANALYSIS TOOL")
    print("="*60)
    
    # Create agent with Galileo logging enabled
    agent = await AwesomeListAgentFactory.create_agent(
        verbosity="high",
        enable_galileo=True
    )
    
    # Sample text for analysis
    sample_text = """
    Python is an amazing programming language that has revolutionized software development. 
    It's known for its simplicity, readability, and extensive library ecosystem. 
    Many developers love Python because it makes coding fun and efficient.
    
    The language is widely used in data science, web development, artificial intelligence, 
    and automation. Companies like Google, Netflix, and Instagram rely heavily on Python 
    for their backend services and data processing pipelines.
    
    Python's community is incredibly supportive and the documentation is excellent. 
    There are countless tutorials, courses, and resources available for learning Python. 
    Whether you're a beginner or an experienced developer, Python has something to offer.
    """
    
    print(f"Analyzing text content ({len(sample_text)} characters)")
    
    try:
        # Use the content analysis tool directly
        analysis_tool = agent.content_analysis_tool
        result = await analysis_tool.execute(
            sample_text,
            analyze_sentiment=True,
            extract_topics=True,
            extract_entities=True,
            analyze_readability=True,
            extract_keywords=True,
            max_topics=5,
            max_keywords=10
        )
        
        if isinstance(result, dict) and "error" not in result:
            print("‚úÖ Successfully analyzed content!")
            print(f"üìä Basic Stats:")
            print(f"  - Words: {result.get('word_count', 0)}")
            print(f"  - Sentences: {result.get('sentence_count', 0)}")
            print(f"  - Paragraphs: {result.get('paragraph_count', 0)}")
            
            # Sentiment analysis
            sentiment = result.get('sentiment', {})
            if sentiment:
                print(f"üòä Sentiment: {sentiment.get('overall', 'N/A')} (score: {sentiment.get('score', 0):.3f})")
                print(f"  - Positive words: {', '.join(sentiment.get('positive_words', [])[:5])}")
                print(f"  - Negative words: {', '.join(sentiment.get('negative_words', [])[:5])}")
            
            # Topics
            topics = result.get('topics', [])
            if topics:
                print(f"üìã Top Topics:")
                for i, topic in enumerate(topics[:3]):
                    print(f"  {i+1}. {topic['topic']} (freq: {topic['frequency']}, score: {topic['relevance_score']:.3f})")
            
            # Keywords
            keywords = result.get('keywords', [])
            if keywords:
                print(f"üîë Top Keywords:")
                for i, keyword in enumerate(keywords[:5]):
                    print(f"  {i+1}. {keyword['keyword']} (freq: {keyword['frequency']}, score: {keyword['tf_idf_score']:.3f})")
            
            # Readability
            readability = result.get('readability', {})
            if readability:
                print(f"üìñ Readability Metrics:")
                print(f"  - Flesch Reading Ease: {readability.get('flesch_reading_ease', 0):.1f}")
                print(f"  - Flesch-Kincaid Grade: {readability.get('flesch_kincaid_grade', 0):.1f}")
                print(f"  - Gunning Fog Index: {readability.get('gunning_fog_index', 0):.1f}")
            
            # Content structure
            structure = result.get('content_structure', {})
            if structure:
                print(f"üìê Content Structure:")
                print(f"  - Has headings: {structure.get('has_headings', False)}")
                print(f"  - Has lists: {structure.get('has_lists', False)}")
                print(f"  - Has links: {structure.get('has_links', False)}")
                print(f"  - Avg sentence length: {structure.get('average_sentence_length', 0):.1f} words")
                print(f"  - Avg word length: {structure.get('average_word_length', 0):.1f} characters")
            
            print(f"üìä Analysis Summary: {result.get('analysis_summary', 'N/A')}")
            
        else:
            print(f"‚ùå Error analyzing content: {result.get('error', 'Unknown error')}")
            
    except Exception as e:
        print(f"‚ùå Exception occurred: {str(e)}")


async def demonstrate_integrated_workflow():
    """Demonstrate how all tools can work together in an integrated workflow."""
    print("\n" + "="*60)
    print("DEMONSTRATING INTEGRATED WORKFLOW")
    print("="*60)
    
    # Create agent with Galileo logging enabled
    agent = await AwesomeListAgentFactory.create_agent(
        verbosity="high",
        enable_galileo=True
    )
    
    # Test with a GitHub README that might contain YouTube links
    test_url = "https://github.com/sindresorhus/awesome"
    
    print(f"Running integrated workflow on: {test_url}")
    print("This will: 1) Parse the awesome list, 2) Scrape content, 3) Analyze the content")
    
    try:
        # Step 1: Parse the awesome list
        print("\nüîç Step 1: Parsing Awesome List...")
        parser_result = await agent.parser.execute(test_url)
        
        if isinstance(parser_result, dict) and "error" not in parser_result:
            print(f"‚úÖ Parsed: {parser_result.get('topic', 'Unknown')} with {parser_result.get('total_items', 0)} items")
            
            # Step 2: Scrape the content for more details
            print("\nüåê Step 2: Scraping web content...")
            scraping_result = await agent.web_scraping_tool.execute(
                test_url,
                extract_text=True,
                extract_links=True,
                extract_metadata=True,
                max_links=20
            )
            
            if isinstance(scraping_result, dict) and "error" not in scraping_result:
                print(f"‚úÖ Scraped: {scraping_result.get('title', 'Unknown')}")
                
                # Step 3: Analyze the scraped content
                print("\nüìä Step 3: Analyzing content...")
                text_content = scraping_result.get('text_content', '')
                if text_content:
                    analysis_result = await agent.content_analysis_tool.execute(
                        text_content[:2000],  # Limit to first 2000 chars for demo
                        analyze_sentiment=True,
                        extract_topics=True,
                        extract_keywords=True,
                        max_topics=5,
                        max_keywords=10
                    )
                    
                    if isinstance(analysis_result, dict) and "error" not in analysis_result:
                        print("‚úÖ Content analysis completed!")
                        
                        # Show integrated results
                        print(f"\nüìã Integrated Results:")
                        print(f"  - Awesome List Topic: {parser_result.get('topic', 'N/A')}")
                        print(f"  - Web Page Title: {scraping_result.get('title', 'N/A')}")
                        print(f"  - Content Sentiment: {analysis_result.get('sentiment', {}).get('overall', 'N/A')}")
                        print(f"  - Key Topics: {', '.join([t['topic'] for t in analysis_result.get('topics', [])[:3]])}")
                        print(f"  - Total Links Found: {len(scraping_result.get('links', []))}")
                        print(f"  - Content Length: {analysis_result.get('word_count', 0)} words")
                        
                        print(f"\nüéØ This demonstrates how all tools can work together to provide comprehensive analysis!")
                    else:
                        print(f"‚ùå Content analysis failed: {analysis_result.get('error', 'Unknown error')}")
                else:
                    print("‚ùå No text content available for analysis")
            else:
                print(f"‚ùå Web scraping failed: {scraping_result.get('error', 'Unknown error')}")
        else:
            print(f"‚ùå Awesome list parsing failed: {parser_result.get('error', 'Unknown error')}")
            
    except Exception as e:
        print(f"‚ùå Exception in integrated workflow: {str(e)}")


async def main():
    """Main function to run all demonstrations."""
    print("üöÄ Awesome List Agent - Comprehensive Tool Demonstration")
    print("This script demonstrates all available tools with Galileo logging enabled.")
    
    # Setup logging
    setup_logging(level="INFO")
    
    # Check if Galileo is enabled
    galileo_enabled = os.getenv("GALILEO_ENABLED", "false").lower() == "true"
    if galileo_enabled:
        print("üìä Galileo observability is ENABLED - all tool executions will be logged")
    else:
        print("üìä Galileo observability is DISABLED - set GALILEO_ENABLED=true to enable")
    
    try:
        # Run all demonstrations
        await demonstrate_awesome_list_parser()
        await demonstrate_youtube_metadata_tool()
        await demonstrate_web_scraping_tool()
        await demonstrate_content_analysis_tool()
        await demonstrate_integrated_workflow()
        
        print("\n" + "="*60)
        print("‚úÖ ALL DEMONSTRATIONS COMPLETED SUCCESSFULLY!")
        print("="*60)
        print("\nüéâ You've successfully seen all tools in action:")
        print("  ‚Ä¢ Awesome List Parser - extracts metadata from awesome lists")
        print("  ‚Ä¢ YouTube Metadata Tool - extracts video information")
        print("  ‚Ä¢ Web Scraping Tool - scrapes and parses web content")
        print("  ‚Ä¢ Content Analysis Tool - analyzes text for insights")
        print("  ‚Ä¢ Integrated Workflow - shows how tools work together")
        
        if galileo_enabled:
            print("\nüìä Check your Galileo dashboard to see detailed execution logs and metrics!")
        
    except Exception as e:
        print(f"\n‚ùå Error running demonstrations: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main()) 